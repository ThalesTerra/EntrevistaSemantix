Qual​ ​o​ ​objetivo​ ​do​ ​comando​ ​​cache​ ​​em​ ​Spark? 
 
 Para armazenar um objeto em memória temporária, para melhor performance no processamento.
 
 
O​ ​mesmo​ ​código​ ​implementado​ ​em​ ​Spark​ ​é​ ​normalmente​ ​mais​ ​rápido​ ​que​ ​a​ ​implementação​ ​equivalente​ ​em MapReduce.​ ​Por​ ​quê? 
 
 Uma das limitações do MapReduce é que ele armazena o dataset inteiro após cara job que ele executa. enquanto o Spark processa o output de um job diretamente em outro job, fazendo com que o processo seja mais rapido como um todo.
 Isso é possivel devido ao armazenamento em memoria volatil conforme o comando cache anteriormente citado.
 
 
Qual​ ​é​ ​a​ ​função​ ​do​ ​​SparkContext​? 
 
 criar um ambiente cliente de acesso ao ambiente de execução, dando a possibilidade de criação e manipulação de objetos dentro do ambiente de execução.
 
 
Explique​ ​com​ ​suas​ ​palavras​ ​​ ​o​ ​que​ ​é​ ​​Resilient​ ​Distributed​ ​Datasets​​ ​(RDD). 
 
 é uma organização logica dos dados dentro do spark, como uma tablespace para um banco.
 RDDs também armazenam objetos fixos que podem ser utilizados por nodes diferentes.
 
 
GroupByKey​ ​​é​ ​menos​ ​eficiente​ ​que​ ​​reduceByKey​ ​​em​ ​grandes​ ​dataset.​ ​Por​ ​quê? 
 
 O ReduceByKey combina os outputs de keys iguais, enquanto o GrouByKey realiza o processo uma a uma independente se são iguais ou diferentes.
 
 
 
 
Este​ ​documento​ ​é​ ​confidencial​ ​e​ ​não​ ​pode​ ​ser​ ​distribuído,​ ​copiado​ ​em​ ​parte​ ​ou​ ​na​ ​sua​ ​totalidade 

 
 
 
Explique​ ​o​ ​que​ ​o​ ​código​ ​Scala​ ​abaixo​ ​faz. 

val​​ ​​textFile​​ ​​=​​ ​​sc​.​textFile​(​"hdfs://..."​) 
val​​ ​​counts​​ ​​=​​ ​​textFile​.​flatMap​(​line​​ ​​=>​​ ​​line​.​split​(​"​ ​"​))
	.​map​(​word​​ ​​=>​​ ​​(​word​,​​ ​​1​))
	.​reduceByKey​(​_​​ ​​+​​ ​​_​)

counts​.​saveAsTextFile​(​"hdfs://..."​) 
 
linha 1: cria o dataset textfile e atribui os dados do endereço no mesmo

linha 2: cria um outro dataset counts para receber as operações map e reducebykey executadas no dataset original textfile

ultima linha: exporta o dataset counts utilizando o comando saveAsTextFile para salvar o dataset como texto.



HTTP​ ​requests​ ​to​ ​the​ ​NASA​ ​Kennedy​ ​Space​ ​Center​ ​WWW​ ​server  
 
Fonte​ ​oficial​ ​do​ ​dateset​:​ ​​http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html Dados​: 
● Jul​ ​01​ ​to​ ​Jul​ ​31,​ ​ASCII​ ​format,​ ​20.7​ ​MB​ ​gzip​ ​compressed​,​ ​205.2​ ​MB. ● Aug​ ​04​ ​to​ ​Aug​ ​31,​ ​ASCII​ ​format,​ ​21.8​ ​MB​ ​gzip​ ​compressed​,​ ​167.8​ ​MB. 

Sobre o dataset​: Esses dois conjuntos de dados possuem todas as requisições HTTP para o servidor da NASA Kennedy                   Space​ ​Center​ ​WWW​ ​na​ ​Flórida​ ​para​ ​um​ ​período​ ​específico.  
 
Os​ ​logs​ ​estão​ ​em​ ​arquivos​ ​ASCII​ ​com​ ​uma​ ​linha​ ​por​ ​requisição​ ​com​ ​as​ ​seguintes​ ​colunas: 
● Host fazendo a requisição​. Um hostname quando possível, caso contrário o endereço de internet se o nome não​ ​puder​ ​ser​ ​identificado. 
● Timestamp​ ​​no​ ​formato​ ​"DIA/MÊS/ANO:HH:MM:SS​ ​TIMEZONE" 
● Requisição​ ​(entre​ ​aspas) 
● Código​ ​do​ ​retorno​ ​HTTP  ● Total​ ​de​ ​bytes​ ​retornados 
 
Questões ​ ​Responda​ ​as​ ​seguintes​ ​questões​ ​devem​ ​ser​ ​desenvolvidas​ ​em​ ​Spark​ ​utilizando​ ​a​ ​sua​ ​linguagem​ ​de​ ​preferência. 
1. Número​ ​de​ ​hosts​ ​únicos. 
2. O​ ​total​ ​de​ ​erros​ ​404. 
3. Os​ ​5​ ​URLs​ ​que​ ​mais​ ​causaram​ ​erro​ ​404. 
4. Quantidade​ ​de​ ​erros​ ​404​ ​por​ ​dia. 
5. O​ ​total​ ​de​ ​bytes​ ​retornados. 
 
 